{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b890b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from expl_perf_drop.explainers import CGExplainerDR\n",
    "from expl_perf_drop.utils import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582e675",
   "metadata": {},
   "source": [
    "## Make Some Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04320f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of nodes in the causal graph to column names in a dataframe.\n",
    "# here, all nodes are scalars, but they can also be vector-valued\n",
    "VAR_CATEGORIES = { \n",
    "    'G': ['G'],\n",
    "    'X1': ['X1'],\n",
    "    'X2': ['X2'],\n",
    "    'X3': ['X3'],\n",
    "    'Y': ['Y']\n",
    "}\n",
    "\n",
    "# define the causal graph\n",
    "GRAPH = Graph(\n",
    "    nodes= list(VAR_CATEGORIES.keys()),\n",
    "    edges=[\n",
    "        ('G', 'Y'),\n",
    "        ('G', 'X2'),\n",
    "        ('G', 'X3'),\n",
    "        ('Y', 'X1'), \n",
    "        ('Y', 'X2'), \n",
    "        ('Y', 'X3'), \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81776b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the synthetic dataset from the appendix of the paper\n",
    "def generate(n, rng, q = 0.9, y_noise = 0.25, mu_add = 3, x1_weight = 1.0):\n",
    "    G = rng.random(size = (n, 1)) >= 0.5\n",
    "    Y = np.logical_xor(G, rng.random(size = (n, 1)) >= q)     \n",
    "    Y_noised_1 = np.logical_xor(Y, rng.random(size = (n, 1)) <= y_noise)\n",
    "    Y_noised_2 = np.logical_xor(Y, rng.random(size = (n, 1)) <= y_noise)   \n",
    "    Y_noised_3 = np.logical_xor(Y, rng.random(size = (n, 1)) <= y_noise)   \n",
    "\n",
    "    X1 = rng.normal(loc = x1_weight * Y_noised_1, size = (n, 1))\n",
    "    X2 = rng.normal(loc = Y_noised_2 + G, size = (n, 1))\n",
    "    X3 = rng.normal(loc = Y_noised_3 + mu_add * G, size = (n, 1))\n",
    "    X = np.concatenate([X1, X2, X3], axis = -1)\n",
    "\n",
    "    return data_to_df(X, G.squeeze(), Y.squeeze())\n",
    "\n",
    "def data_to_df(X, G, Y):\n",
    "    df = pd.DataFrame({'G': G.astype(int), 'Y': Y.astype(int)})\n",
    "    for i in range(1, X.shape[1] + 1):\n",
    "        df[f'X{i}'] = X[:, i-1]\n",
    "    return df\n",
    "\n",
    "# make dataframes for source and target domains\n",
    "source_df = generate(20000, np.random.RandomState(0))\n",
    "source_train_df, source_eval_df = train_test_split(source_df, random_state = 0, shuffle = True, test_size = 0.25)\n",
    "\n",
    "target_df = generate(20000, np.random.RandomState(1), q = 0.1)\n",
    "target_train_df, target_eval_df = train_test_split(target_df, random_state = 1, shuffle = True, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbd87e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18960</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.769550</td>\n",
       "      <td>-0.616149</td>\n",
       "      <td>0.663881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11353</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.166020</td>\n",
       "      <td>0.860702</td>\n",
       "      <td>1.968601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7449</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.070470</td>\n",
       "      <td>0.378767</td>\n",
       "      <td>-0.065953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14308</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.510399</td>\n",
       "      <td>-0.016011</td>\n",
       "      <td>2.110591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.446860</td>\n",
       "      <td>1.046087</td>\n",
       "      <td>0.410305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>1.400120</td>\n",
       "      <td>2.647361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.092225</td>\n",
       "      <td>0.876356</td>\n",
       "      <td>2.781320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.587095</td>\n",
       "      <td>-0.498254</td>\n",
       "      <td>2.966031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156151</td>\n",
       "      <td>2.756553</td>\n",
       "      <td>4.318010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753814</td>\n",
       "      <td>1.660155</td>\n",
       "      <td>1.968772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       G  Y        X1        X2        X3\n",
       "18960  0  1  0.769550 -0.616149  0.663881\n",
       "11353  0  1 -1.166020  0.860702  1.968601\n",
       "7449   0  1 -0.070470  0.378767 -0.065953\n",
       "14308  1  0  0.510399 -0.016011  2.110591\n",
       "11889  0  1  0.446860  1.046087  0.410305\n",
       "...   .. ..       ...       ...       ...\n",
       "10955  1  0  0.529000  1.400120  2.647361\n",
       "17289  1  0 -1.092225  0.876356  2.781320\n",
       "5192   1  0  1.587095 -0.498254  2.966031\n",
       "12172  1  0 -0.156151  2.756553  4.318010\n",
       "235    1  0 -0.753814  1.660155  1.968772\n",
       "\n",
       "[15000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all dataframes must have column names corresponding to all values in the VAR_CATEGORIES mapping\n",
    "target_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4020f5",
   "metadata": {},
   "source": [
    "## Training/Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use an XGBoost model, but any model can be used as long as you have a compatible metric function\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "TRAIN_FEATURES = ['X1', 'X2', 'X3']\n",
    "\n",
    "model = GridSearchCV(\n",
    "        estimator = XGBClassifier(random_state = 42, n_jobs = -1),\n",
    "        param_grid = {'max_depth': [1, 2, 3, 4, 5]},\n",
    "        scoring = 'roc_auc_ovr',\n",
    "        cv = 3,\n",
    "        refit = True\n",
    "    ).fit(source_train_df[TRAIN_FEATURES], source_train_df['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a61bb6",
   "metadata": {},
   "source": [
    "## Define the Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1425fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a metric function takes in:\n",
    "## in a model, \n",
    "## a dataframe, \n",
    "## a set of column names to be passed into the model,\n",
    "## a vector of weights,\n",
    "## and potentially a target name,\n",
    "## and returns a scalar.\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# implemented metrics for sklearn models can also be found in expl_perf_drop.metrics\n",
    "def brier(model, data, subset_cols = None, weight = None, target_name = 'Y'):\n",
    "    data_input = data[subset_cols] if subset_cols is not None else data\n",
    "    return brier_score_loss(data[target_name], model.predict_proba(data_input)[:, 1], sample_weight = weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1bf1b",
   "metadata": {},
   "source": [
    "## Getting the Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc47f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = CGExplainerDR(GRAPH, source_train_df, source_eval_df, target_train_df, target_eval_df,\n",
    "        TRAIN_FEATURES, VAR_CATEGORIES, target_name = 'Y'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd53eacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 0.09999182363046939, 'target': 0.5564098680748968}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance of your model on the source and target\n",
    "perfs = exp.get_perf_on_sets(model, metric = brier)\n",
    "perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27a2c920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G',\n",
       " (frozenset({'Y'}), 'X1'),\n",
       " (frozenset({'G', 'Y'}), 'X2'),\n",
       " (frozenset({'G', 'Y'}), 'X3'),\n",
       " (frozenset({'G'}), 'Y')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all possilble shifts which the graph entails\n",
    "exp.get_all_possible_shifts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44a5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate set function: 32it [00:00, 64965.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P(G)           0.000044\n",
       "P(X1 | Y)      0.000211\n",
       "P(X2 | G,Y)   -0.014848\n",
       "P(X3 | G,Y)    0.003463\n",
       "P(Y | G)       0.448967\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the explanation!\n",
    "res = exp.explain(model, metric = brier)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3447d52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P(G)           0.000046\n",
       "P(X1 | Y)      0.000220\n",
       "P(X2 | G,Y)   -0.015478\n",
       "P(X3 | G,Y)    0.003610\n",
       "P(Y | G)       0.468020\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optionally, can re-scale the explanation to ensure they sum to the total shift (which may not happen due to estimation error)\n",
    "exp.scale(res, perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81559d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
